# ===== PARAMETERS (pipeline overrides these) =====
target_workspace_id = ""          # optional. blank = current workspace
target_sqldb_name   = "MySqlDb"   # Fabric SQL database item name
sql_folder          = "/lakehouse/default/Files/dbscripts"
sql_file_name       = "001_init.sql"





import re
from IPython import get_ipython

# Read one .sql file from the Lakehouse Files area
path = f"{sql_folder.rstrip('/')}/{sql_file_name}"
with open(path, "r", encoding="utf-8-sig") as f:
    sql_text = f.read()

# Split on GO (batch separator)
batches = [b.strip() for b in re.split(r"(?im)^\s*GO\s*;?\s*$", sql_text) if b.strip()]

# Build tsql magic args (workspace optional)
args = f'-artifact "{target_sqldb_name}" -type SQLDatabase'
if target_workspace_id:
    args += f' -workspace {target_workspace_id}'

ip = get_ipython()

# Execute each batch against the target SQL database
for i, batch in enumerate(batches, start=1):
    ip.run_cell_magic("tsql", args, batch)
    print(f"OK: {sql_file_name} batch {i}/{len(batches)}")




Sanity Check :

from IPython import get_ipython
ip = get_ipython()

args = f'-artifact "{target_sqldb_name}" -type SQLDatabase'
if target_workspace_id:
    args += f' -workspace {target_workspace_id}'

ip.run_cell_magic("tsql", args, "SELECT DB_NAME() AS CurrentDb;")





Base parameters Examples:

sql_file_name = @item().name

sql_folder = /lakehouse/default/Files/dbscripts

target_sqldb_name = @pipeline().parameters.SqlDbName

target_workspace_id = @pipeline().parameters.TargetWorkspaceId (blank for same workspace)








